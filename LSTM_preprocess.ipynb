{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import csv\n",
    "import time\n",
    "import tqdm\n",
    "import nltk\n",
    "import math\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "from sklearn import datasets\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import *\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_selection import chi2\n",
    "from scipy.stats import uniform\n",
    "from scipy.stats import randint\n",
    "#from spellchecker import SpellChecker #need to install for some computers\n",
    "import random\n",
    "#from Test import * \n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "\n",
    "stops = [\"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\",\\\n",
    "         \"again\", \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\",\\\n",
    "         \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\",\\\n",
    "         \"another\", \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\", \"as\",\\\n",
    "         \"at\", \"be\", \"became\", \"because\", \"become\",\"becomes\", \"becoming\", \"been\", \"before\", \\\n",
    "         \"behind\", \"being\", \"beside\", \"besides\", \"between\", \"beyond\", \"both\", \"but\", \"by\",\"can\",\\\n",
    "         'cannot', \"cant\", \"could\", \"couldnt\", \"de\", \"describe\", \"do\", \"done\", \"each\", \"eg\",'either', \"else\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\", \"everything\",\"everywhere\", \"except\", \"few\", \"find\",\"for\",\"found\", \"four\", \"from\", \"further\", \"get\",\"give\", \"go\", \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\",\"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\",\"however\", \"i\", \"ie\", \"if\", \"in\", \"indeed\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"least\",\"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mine\", \"more\", \"moreover\",\"most\", \"mostly\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"never\", \\\n",
    "         \"nevertheless\", \"next\",\"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\",\"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \\\n",
    "         \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\",\"perhaps\",\"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"she\",\"should\",\"since\", \"sincere\",\"so\", \"some\", \"somehow\", \"someone\", \"something\", \"sometime\",\"sometimes\", \"somewhere\", \"still\", \"such\", \"take\",\"than\", \"that\", \"the\", \"their\", \"them\", \\\n",
    "         \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"therefore\", \"therein\",\"thereupon\", \"these\", \"they\",\"this\", \"those\", \"though\", \"through\", \"throughout\",\"thru\", \"thus\", \"to\", \"together\", \"too\", \"toward\", \"towards\",\"under\", \"until\", \"up\", \"upon\", \"us\",\\\n",
    "         \"very\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\",\\\n",
    "         \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\",\\\n",
    "         \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \\\n",
    "         \"who\", \"whoever\", \"whom\", \"whose\", \"why\", \"will\", \"with\",\\\n",
    "         \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# feature_names = []\n",
    "\n",
    "# vectorizer = TfidfVectorizer(ngram_range=(1,2),stop_words='english',norm='l2',max_df = 0.995,min_df=2,sublinear_tf=False)\n",
    "# #vectorizer = CountVectorizer(ngram_range=(1,1),stop_words='english',binary=True)\n",
    "\n",
    "# corpus_train = pd.read_csv(\"reddit_train.csv\",usecols=['comments','subreddits'],delimiter=',',sep='\\s*,\\s*')\n",
    "# corpus_test = pd.read_csv(\"reddit_train.csv\",usecols=['comments','subreddits'],delimiter=',',sep='\\s*,\\s*')\n",
    "# #random.shuffle(corpus_train)\n",
    "# #corpus_test = corpus_train[\"comments\"]\n",
    "\n",
    "# #corpus_test = corpus_train[60000:]\n",
    "# corpus_train = shuffle(corpus_train)\n",
    "# corpus_test = corpus_train[:10000]\n",
    "# corpus_train = corpus_train[10000:]\n",
    "\n",
    "\n",
    "\n",
    "english_words = set(nltk.corpus.words.words()\\\n",
    "                    + nltk.corpus.gutenberg.words()\\\n",
    "                    + nltk.corpus.webtext.words()\\\n",
    "                    + nltk.corpus.nps_chat.words()\\\n",
    "                    + nltk.corpus.brown.words() + nltk.corpus.reuters.words())\n",
    "\n",
    "           \n",
    "def misspell(x):\n",
    "    if \"aaa\" not in x and \"bbb\" not in x and \"ccc\" not in x and \"ddd\" not in x \\\n",
    "    and \"eee\" not in x and \"fff\" not in x and \"ggg\" not in x and \"hhh\" not in x \\\n",
    "    and \"iii\" not in x and \"jjj\" not in x and \"kkk\" not in x and \"lll\" not in x and \"mmm\" not in x and \"nnn\" not in x \\\n",
    "    and \"ooo\" not in x and \"ppp\" not in x and \"qqq\" not in x and \"rrr\" not in x and \"sss\" not in x and \"ttt\" not in x \\\n",
    "    and \"uuu\" not in x and \"vvv\" not in x and \"www\" not in x and \"xxx\" not in x and \"yyy\" not in x and \"zzz\" not in x: #\\\n",
    "    #and \"aa\" not in x and \"zz\" not in x:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "\n",
    "# delete_list = [\"aaa\" not in x,\"bbb\" not in x,\"ccc\" not in x,\"ddd\" not in x,\"eee\" not in x,\"fff\" not in x,\"ggg\" not in x,\\\n",
    "#                \"hhh\" not in x,\"iii\" not in x,\"jjj\" not in x,\"kkk\" not in x,\"lll\" not in x,\"mmm\" not in x,\"nnn\" not in x,\\\n",
    "#                \"ooo\" not in x,\"ppp\" not in x,\"qqq\" not in x,\"rrr\" not in x,\"sss\" not in x,\"ttt\" not in x\n",
    "#                ,\"uuu\" not in x,\"vvv\" not in x,\"www\" not in x,\"xxx\" not in x,\"yyy\" not in x,\"zzz\" not in x]\n",
    "\n",
    "# a helper function to process one comment\n",
    "def preprocess_text(text): \n",
    "    text = text.lower().split()\n",
    "    #stops = set(stopwords.words(\"english\"))\n",
    "#     text = [w for w in text if not w in stops and len(w) >= 3]\n",
    "    text = \" \".join(text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"^https?:\\/\\/.*[\\r\\n]*\",\"\", text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ',text)\n",
    "    text = text.split()\n",
    "    \n",
    "    # lemmatization\n",
    "    lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "    text = [lemma.lemmatize(word) for word in text]\n",
    "    \n",
    "#     # stemming\n",
    "#     text = [PorterStemmer().stem(word) for word in text]\n",
    "    \n",
    "    text_final = []\n",
    "    \n",
    "    # clean all non-English words, numbers, and other weirdos, stopwords\n",
    "    for x in text:\n",
    "        #x = spell.correction(x)\n",
    "        if x.isalpha() and len(x)<20 and len(x) > 1 and misspell(x) and x not in stops: #and is_english_word(x):\n",
    "#             if len(x) >10:\n",
    "#                 text_final.append(PorterStemmer().stem(x))\n",
    "#             else:\n",
    "            text_final.append(x)\n",
    "    \n",
    "    text = \" \".join(text_final)\n",
    "    return text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
